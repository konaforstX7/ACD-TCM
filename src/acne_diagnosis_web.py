import copy
import re
import os
import glob
from argparse import ArgumentParser
from threading import Thread
import gc

import gradio as gr
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, TextIteratorStreamer
from peft import PeftModel

DEFAULT_CKPT_PATH = '/root/autodl-tmp/Qwen/Qwen2.5-VL-7B-Instruct'
TRAINING_OUTPUT_DIR = '/root/autodl-tmp/Qwen/Qwen2.5-VL/output/Qwen2_5-VL-7B-Acne'

# ç—¤ç–®è¯Šæ–­ä¸“ç”¨promptæ¨¡æ¿
ACNE_DIAGNOSIS_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­åŒ»çš®è‚¤ç§‘åŒ»ç”Ÿï¼Œæ“…é•¿ç—¤ç–®çš„è¯Šæ–­å’Œæ²»ç–—ã€‚è¯·æ ¹æ®æä¾›çš„é¢éƒ¨å›¾åƒï¼Œè¿›è¡Œè¯¦ç»†çš„ç—¤ç–®åˆ†æå’Œè¯Šæ–­ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡Œåˆ†æï¼š

1. **ç—¤ç–®ç±»å‹è¯†åˆ«**ï¼š
   - è¯†åˆ«å›¾åƒä¸­çš„ç—¤ç–®ç±»å‹ï¼ˆå¦‚ï¼šç²‰åˆºã€ä¸˜ç–¹ã€è„“ç–±ã€ç»“èŠ‚ã€å›Šè‚¿ç­‰ï¼‰
   - æè¿°ç—¤ç–®çš„åˆ†å¸ƒä½ç½®å’Œä¸¥é‡ç¨‹åº¦

2. **ä¸­åŒ»è¾¨è¯åˆ†æ**ï¼š
   - æ ¹æ®ç—¤ç–®çš„è¡¨ç°ç‰¹å¾è¿›è¡Œä¸­åŒ»è¯å‹åˆ†æ
   - å¯èƒ½çš„è¯å‹åŒ…æ‹¬ï¼šè‚ºç»é£çƒ­ã€è„¾èƒƒæ¹¿çƒ­ã€å†²ä»»ä¸è°ƒã€ç—°æ¹¿å‡ç»“ã€è¡€ç˜€ç—°ç»“ç­‰

3. **è¯Šæ–­ç»“è®º**ï¼š
   - ç»™å‡ºæ˜ç¡®çš„ç—¤ç–®è¯Šæ–­
   - è¯„ä¼°ä¸¥é‡ç¨‹åº¦ï¼ˆè½»åº¦ã€ä¸­åº¦ã€é‡åº¦ï¼‰

4. **æ²»ç–—å»ºè®®**ï¼š
   - ä¸­åŒ»æ²»ç–—æ–¹æ¡ˆï¼ˆå¦‚ä¸­è¯æ–¹å‰‚ã€é’ˆç¸ç­‰ï¼‰
   - æ—¥å¸¸æŠ¤ç†å»ºè®®
   - é¥®é£Ÿè°ƒç†å»ºè®®

è¯·åŸºäºå›¾åƒå†…å®¹è¿›è¡Œä¸“ä¸šã€è¯¦ç»†çš„åˆ†æã€‚"""

def _get_args():
    parser = ArgumentParser()
    parser.add_argument('-c',
                        '--checkpoint-path',
                        type=str,
                        default=DEFAULT_CKPT_PATH,
                        help='Base model checkpoint path')
    parser.add_argument('--cpu-only', action='store_true', help='Run demo with CPU only')
    parser.add_argument('--flash-attn2',
                        action='store_true',
                        default=False,
                        help='Enable flash_attention_2 when loading the model.')
    parser.add_argument('--share',
                        action='store_true',
                        default=False,
                        help='Create a publicly shareable link for the interface.')
    parser.add_argument('--inbrowser',
                        action='store_true',
                        default=False,
                        help='Automatically launch the interface in a new tab on the default browser.')
    parser.add_argument('--server-port', type=int, default=7860, help='Demo server port.')
    parser.add_argument('--server-name', type=str, default='0.0.0.0', help='Demo server name.')
    args = parser.parse_args()
    return args

def get_available_checkpoints():
    """è·å–å¯ç”¨çš„checkpointåˆ—è¡¨"""
    checkpoints = []
    
    # æ·»åŠ åŸºç¡€æ¨¡å‹
    checkpoints.append(("åŸºç¡€æ¨¡å‹ (Base Model)", DEFAULT_CKPT_PATH))
    
    # æŸ¥æ‰¾è®­ç»ƒè¾“å‡ºç›®å½•ä¸­çš„checkpoint
    if os.path.exists(TRAINING_OUTPUT_DIR):
        checkpoint_dirs = glob.glob(os.path.join(TRAINING_OUTPUT_DIR, "checkpoint-*"))
        checkpoint_dirs.sort(key=lambda x: int(x.split('-')[-1]))
        
        for checkpoint_dir in checkpoint_dirs:
            step_num = checkpoint_dir.split('-')[-1]
            display_name = f"å¾®è°ƒæ¨¡å‹ Step {step_num}"
            checkpoints.append((display_name, checkpoint_dir))
    
    return checkpoints

def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def _load_model_processor(base_model_path, checkpoint_path=None, cpu_only=False, flash_attn2=False):
    """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
    # æ¸…ç†å†…å­˜
    clear_gpu_memory()
    
    if cpu_only:
        device_map = 'cpu'
    else:
        device_map = 'auto'

    # åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨å†…å­˜ä¼˜åŒ–é€‰é¡¹
    if flash_attn2:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦å‡å°‘å†…å­˜å ç”¨
            attn_implementation='flash_attention_2',
            device_map=device_map,
            low_cpu_mem_usage=True     # å‡å°‘CPUå†…å­˜ä½¿ç”¨
        )
    else:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            base_model_path, 
            device_map=device_map,
            torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦å‡å°‘å†…å­˜å ç”¨
            low_cpu_mem_usage=True     # å‡å°‘CPUå†…å­˜ä½¿ç”¨
        )

    # å¦‚æœæŒ‡å®šäº†checkpointè·¯å¾„ä¸”ä¸æ˜¯åŸºç¡€æ¨¡å‹ï¼Œåˆ™åŠ è½½LoRAæƒé‡
    if checkpoint_path and checkpoint_path != base_model_path:
        print(f"Loading LoRA weights from: {checkpoint_path}")
        model = PeftModel.from_pretrained(model, checkpoint_path)
        model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡

    processor = AutoProcessor.from_pretrained(base_model_path)
    return model, processor

def _parse_text(text):
    lines = text.split('\n')
    lines = [line for line in lines if line != '']
    count = 0
    for i, line in enumerate(lines):
        if '```' in line:
            count += 1
            items = line.split('`')
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = '<br></code></pre>'
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace('`', r'\`')
                    line = line.replace('<', '&lt;')
                    line = line.replace('>', '&gt;')
                    line = line.replace(' ', '&nbsp;')
                    line = line.replace('*', '&ast;')
                    line = line.replace('_', '&lowbar;')
                    line = line.replace('-', '&#45;')
                    line = line.replace('.', '&#46;')
                    line = line.replace('!', '&#33;')
                    line = line.replace('(', '&#40;')
                    line = line.replace(')', '&#41;')
                    line = line.replace('$', '&#36;')
                lines[i] = '<br>' + line
    text = ''.join(lines)
    return text

def _remove_image_special(text):
    text = text.replace('<ref>', '').replace('</ref>', '')
    return re.sub(r'<box>.*?(</box>|$)', '', text)

def _gc():
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def _transform_messages(original_messages):
    transformed_messages = []
    for message in original_messages:
        new_content = []
        for item in message['content']:
            if 'image' in item:
                new_item = {'type': 'image', 'image': item['image']}
            elif 'text' in item:
                new_item = {'type': 'text', 'text': item['text']}
            else:
                continue
            new_content.append(new_item)

        new_message = {'role': message['role'], 'content': new_content}
        transformed_messages.append(new_message)

    return transformed_messages

def _launch_demo(args):
    # è·å–å¯ç”¨çš„checkpoints
    available_checkpoints = get_available_checkpoints()
    
    # å…¨å±€å˜é‡å­˜å‚¨å½“å‰æ¨¡å‹
    current_model = None
    current_processor = None
    current_checkpoint = None

    def load_checkpoint(checkpoint_choice):
        nonlocal current_model, current_processor, current_checkpoint
        
        # æ‰¾åˆ°é€‰æ‹©çš„checkpointè·¯å¾„
        selected_checkpoint = None
        for display_name, checkpoint_path in available_checkpoints:
            if display_name == checkpoint_choice:
                selected_checkpoint = checkpoint_path
                break
        
        if selected_checkpoint is None:
            return "âŒ æœªæ‰¾åˆ°é€‰æ‹©çš„checkpoint"
        
        if current_checkpoint == selected_checkpoint:
            return f"âœ… å½“å‰å·²åŠ è½½: {checkpoint_choice}"
        
        try:
            # æ¸…ç†ä¹‹å‰çš„æ¨¡å‹å’ŒGPUå†…å­˜
            if current_model is not None:
                del current_model
                del current_processor
                _gc()
                clear_gpu_memory()
            
            # åŠ è½½æ–°æ¨¡å‹
            current_model, current_processor = _load_model_processor(
                args.checkpoint_path,
                selected_checkpoint,
                args.cpu_only,
                args.flash_attn2
            )
            current_checkpoint = selected_checkpoint
            
            return f"âœ… æˆåŠŸåŠ è½½: {checkpoint_choice}"
        except torch.cuda.OutOfMemoryError as e:
            clear_gpu_memory()
            return f"âŒ GPUå†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚è¯·å°è¯•ï¼š\n1. é‡å¯ç¨‹åº\n2. å…³é—­å…¶ä»–å ç”¨GPUçš„ç¨‹åº\né”™è¯¯è¯¦æƒ…: {str(e)}"
        except Exception as e:
            clear_gpu_memory()
            return f"âŒ åŠ è½½å¤±è´¥: {str(e)}"

    def call_local_model(messages):
        if current_model is None or current_processor is None:
            yield "è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½ä¸€ä¸ªcheckpointæ¨¡å‹"
            return

        try:
            # æ¸…ç†GPUå†…å­˜
            clear_gpu_memory()
            
            messages = _transform_messages(messages)
            
            # ä½¿ç”¨torch.no_grad()å‡å°‘å†…å­˜å ç”¨
            with torch.no_grad():
                text = current_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                image_inputs, video_inputs = process_vision_info(messages)
                inputs = current_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors='pt')
                inputs = inputs.to(current_model.device)

                tokenizer = current_processor.tokenizer
                streamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)

                # å‡å°‘max_new_tokensä»¥èŠ‚çœå†…å­˜
                gen_kwargs = {
                    'max_new_tokens': 512,  # å‡å°‘ç”Ÿæˆé•¿åº¦ä»¥èŠ‚çœå†…å­˜
                    'streamer': streamer, 
                    'pad_token_id': tokenizer.eos_token_id,
                    **inputs
                }

                thread = Thread(target=current_model.generate, kwargs=gen_kwargs)
                thread.start()

                generated_text = ''
                for new_text in streamer:
                    generated_text += new_text
                    yield generated_text
                    
            # æ¸…ç†å†…å­˜
            del inputs
            clear_gpu_memory()
            
        except torch.cuda.OutOfMemoryError as e:
            clear_gpu_memory()
            yield f"GPUå†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š\n1. é‡å¯Webç•Œé¢\n2. ä½¿ç”¨æ›´å°çš„å›¾ç‰‡\n3. å…³é—­å…¶ä»–å ç”¨GPUçš„ç¨‹åº\né”™è¯¯è¯¦æƒ…: {str(e)}"
        except Exception as e:
            clear_gpu_memory()
            yield f"ç”Ÿæˆå¤±è´¥: {str(e)}"

    def predict_acne(_chatbot, task_history, use_template):
        if not task_history:
            return _chatbot
        
        chat_query = _chatbot[-1][0]
        query = task_history[-1][0]
        
        if len(chat_query) == 0:
            _chatbot.pop()
            task_history.pop()
            return _chatbot
        
        print('User: ' + _parse_text(str(query)))
        
        # æ„å»ºæ¶ˆæ¯
        messages = []
        content = []
        
        # æ·»åŠ å›¾åƒ
        if isinstance(query, (tuple, list)):
            content.append({'image': f'file://{query[0]}'})
        
        # æ·»åŠ æ–‡æœ¬prompt
        if use_template:
            content.append({'text': ACNE_DIAGNOSIS_PROMPT})
        else:
            # å¦‚æœä¸ä½¿ç”¨æ¨¡æ¿ï¼Œæ·»åŠ ç®€å•çš„è¯Šæ–­è¯·æ±‚
            content.append({'text': 'è¯·åˆ†æè¿™å¼ é¢éƒ¨å›¾åƒä¸­çš„ç—¤ç–®æƒ…å†µã€‚'})
        
        messages.append({'role': 'user', 'content': content})
        
        # ç”Ÿæˆå›å¤
        for response in call_local_model(messages):
            _chatbot[-1] = (_parse_text(str(chat_query)), _remove_image_special(_parse_text(response)))
            yield _chatbot
        
        task_history[-1] = (query, _parse_text(response))
        print('Acne Diagnosis: ' + _parse_text(response))
        yield _chatbot

    def add_file(history, task_history, file):
        history = history if history is not None else []
        task_history = task_history if task_history is not None else []
        history = history + [((file.name,), None)]
        task_history = task_history + [((file.name,), None)]
        return history, task_history

    def reset_state(_chatbot, task_history):
        task_history.clear()
        _chatbot.clear()
        _gc()
        return []

    # åˆ›å»ºGradioç•Œé¢
    with gr.Blocks(title="ç—¤ç–®è¯Šæ–­AIåŠ©æ‰‹") as demo:
        gr.Markdown("""
        # ğŸ¥ ç—¤ç–®è¯Šæ–­AIåŠ©æ‰‹
        
        åŸºäºQwen2.5-VLçš„ä¸­åŒ»ç—¤ç–®è¯Šæ–­ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§å¾®è°ƒæ¨¡å‹é€‰æ‹©ã€‚
        
        ## ä½¿ç”¨è¯´æ˜ï¼š
        1. é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹checkpoint
        2. ä¸Šä¼ é¢éƒ¨ç—¤ç–®å›¾åƒ
        3. é€‰æ‹©æ˜¯å¦ä½¿ç”¨ä¸“ä¸šè¯Šæ–­æ¨¡æ¿
        4. ç‚¹å‡»è¯Šæ–­æŒ‰é’®è·å–åˆ†æç»“æœ
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # Checkpointé€‰æ‹©
                checkpoint_choices = [name for name, _ in available_checkpoints]
                checkpoint_dropdown = gr.Dropdown(
                    choices=checkpoint_choices,
                    label="é€‰æ‹©æ¨¡å‹ Checkpoint",
                    value=checkpoint_choices[0] if checkpoint_choices else None
                )
                
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                load_status = gr.Textbox(label="åŠ è½½çŠ¶æ€", interactive=False)
                
                # è¯Šæ–­é€‰é¡¹
                use_template = gr.Checkbox(
                    label="ä½¿ç”¨ä¸“ä¸šè¯Šæ–­æ¨¡æ¿",
                    value=True,
                    info="å¯ç”¨åå°†ä½¿ç”¨è¯¦ç»†çš„ä¸­åŒ»è¯Šæ–­prompt"
                )
            
            with gr.Column(scale=2):
                # èŠå¤©ç•Œé¢
                chatbot = gr.Chatbot(
                    label='ç—¤ç–®è¯Šæ–­ç»“æœ',
                    elem_classes='control-height',
                    height=400
                )
                
                task_history = gr.State([])
                
                with gr.Row():
                    upload_btn = gr.UploadButton(
                        "ğŸ“· ä¸Šä¼ é¢éƒ¨å›¾åƒ",
                        file_types=['image'],
                        variant="secondary"
                    )
                    diagnose_btn = gr.Button("ğŸ” å¼€å§‹è¯Šæ–­", variant="primary")
                    clear_btn = gr.Button("ğŸ§¹ æ¸…é™¤å†å²", variant="secondary")
        
        # äº‹ä»¶ç»‘å®š
        load_btn.click(
            load_checkpoint,
            inputs=[checkpoint_dropdown],
            outputs=[load_status]
        )
        
        upload_btn.upload(
            add_file,
            inputs=[chatbot, task_history, upload_btn],
            outputs=[chatbot, task_history]
        )
        
        diagnose_btn.click(
            predict_acne,
            inputs=[chatbot, task_history, use_template],
            outputs=[chatbot],
            show_progress=True
        )
        
        clear_btn.click(
            reset_state,
            inputs=[chatbot, task_history],
            outputs=[chatbot]
        )
        
        gr.Markdown("""
        ---
        
        **æ³¨æ„äº‹é¡¹ï¼š**
        - æœ¬ç³»ç»Ÿçš„ç»“æœä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
        - è¯·ä¸Šä¼ æ¸…æ™°çš„é¢éƒ¨å›¾åƒä»¥è·å¾—æ›´å‡†ç¡®çš„åˆ†æ
        - å»ºè®®ç»“åˆä¸“ä¸šåŒ»ç”Ÿçš„æ„è§è¿›è¡Œç»¼åˆåˆ¤æ–­
        
        
        """)
    
    # å¯åŠ¨demo
    demo.queue().launch(
        share=args.share,
        inbrowser=args.inbrowser,
        server_port=args.server_port,
        server_name=args.server_name,
    )

def main():
    args = _get_args()
    _launch_demo(args)

if __name__ == '__main__':
    main()